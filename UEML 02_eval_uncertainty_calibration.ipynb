{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoVcs7-M31hw",
        "outputId": "aca9100b-61e9-4865-cacb-65c14af2fd27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/mvaldenegro/keras-uncertainty.git\n",
            "  Cloning https://github.com/mvaldenegro/keras-uncertainty.git to /tmp/pip-req-build-so1bp5fb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mvaldenegro/keras-uncertainty.git /tmp/pip-req-build-so1bp5fb\n",
            "  Resolved https://github.com/mvaldenegro/keras-uncertainty.git to commit 42f50a36c70003b16b7f343002766708ad2a289a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from Keras-Uncertainty==0.0.1) (2.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from Keras-Uncertainty==0.0.1) (1.23.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from Keras-Uncertainty==0.0.1) (4.66.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from Keras-Uncertainty==0.0.1) (1.11.4)\n",
            "Building wheels for collected packages: Keras-Uncertainty\n",
            "  Building wheel for Keras-Uncertainty (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Keras-Uncertainty: filename=Keras_Uncertainty-0.0.1-py3-none-any.whl size=37767 sha256=c495431cdf757a3a0c50627ab9ca820fdd1de5680c1770948d5b8243ae437ae9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yzj0zpu6/wheels/ca/4e/2f/97609d4065ffb545a5e867c8b31430161987f4702ecf61a96d\n",
            "Successfully built Keras-Uncertainty\n",
            "Installing collected packages: Keras-Uncertainty\n",
            "Successfully installed Keras-Uncertainty-0.0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Keras Uncertainty will use standalone Keras backend"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/mvaldenegro/keras-uncertainty.git\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "import matplotlib.transforms as mtransforms\n",
        "import keras\n",
        "import keras_uncertainty as ku"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to use throughout the lab\n",
        "def download_weights(url, destination_filename):\n",
        "  request = requests.get(url)\n",
        "  with open(destination_filename, 'wb') as outfile:\n",
        "      outfile.write(request.content)"
      ],
      "metadata": {
        "id": "S_SHUp1X-W8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the performance of probabilistic models\n",
        "\n",
        "We first implement two metrics for Bayesian models:\n",
        "\n",
        "* **Coverage** for regression\n",
        "* **Brier Score** for classification\n",
        "\n",
        "Note: in the definitions used below, we will have the following entities:\n",
        "\n",
        "* $y\\in\\mathbb{R}^N$ is the vector containing the ground truth.\n",
        "* $\\hat{y}$ is a vector of $M$ probability distributions (the predictive posterior probabilities $q(y|x, \\theta)$), $x$ being a set of $N$ data points and $\\theta$ the model parameters. We may have a close form of these distributions, or samples obtained, e.g., using Monte-Carlo methods. In the former case, the predictions will be a $M \\times N$ matrix which we call $\\hat{Y}$.\n",
        "\n",
        "  We can thus calculate the per-datapoint mean and standard deviations:\n",
        "  $$\n",
        "  \\mu_{\\hat{Y}}^{(i)} = \\frac{\\sum_{j=1}^M \\hat{y}^{(i)}_j}{M} \\\\\n",
        "  \\sigma_{\\hat{Y}}^{(i)} = \\sqrt{\\frac{\\sum_{j=1}^M \\left(\\hat{y}^{(i)}_{j} - \\mu_{\\hat{Y}}^{(i)}\\right)^2}{M}}\n",
        "  $$\n",
        "\n",
        "\n",
        "For each datapoint, we can hence construct a confidence interval, e.g., by summing and subtracting 2*std from the mean prediction:\n",
        "$$\n",
        "\\text{CI}_i = \\left[\\mu_{\\hat{Y}}^{(i)} - 2\\sigma_{\\hat{Y}}^{(i)}, \\mu_{\\hat{Y}}^{(i)} + 2\\sigma_{\\hat{Y}}^{(i)}\\right]\n",
        "$$\n",
        "\n",
        "Note: we will implement these metrics using NumPy: these can be further adapted to Pytorch or Tensorflow\n"
      ],
      "metadata": {
        "id": "a_-QFjLY3_l3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coverage\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=191dtIqeVOyPhLBJXxzItaJwyCQNaIUA1)\n",
        "\n",
        "Coverage calculates whether the confidence interval contains the ground truth for each of the datapoints within the dataset. It is hence an extension of accuracy for probabilistic models for regression.\n",
        "\n",
        "$$\n",
        "\\text{coverage}(y,\\hat{y}) = \\frac{\\sum_{i=1}^{N} \\mathbb {1}_{\\left[ y^{(i)} \\in \\text{CI}_i   \\right]}}{N}\n",
        "$$\n",
        "\n",
        "Note: $\\mathbb{1}_{[\\text{condition}]}$ is the **indicator function**, which returns 1 if the condition in brackets is true, 0 if false:\n",
        "\n",
        "$$\n",
        "\\mathbb{1}_{[\\text{condition}]} =\n",
        "  \\begin{cases}\n",
        "    1 &\\text{if condition true} \\\\\n",
        "    0 &\\text{otherwise}\n",
        "  \\end{cases}\n",
        "$$\n",
        "\n",
        "**Your task**:\n",
        "you are given `y_true`, the ground truth of a regression model (a NumPy ndarray of size `(N,)`) and `y_pred`, the prediction output of a `StochasticRegressor` model from `keras_uncertainty`.\n",
        "\n",
        "_Tips_: you can find the weights of a pretrained MCDropout regressor on the GitHub page, under the folder `files`, its name is `mcdropout_regression_weights.keras`. The corresponding architecture is the following:\n",
        "\n",
        "```python\n",
        "backbone_dropout = keras.models.Sequential([\n",
        "    keras.Input(shape=(1,)),\n",
        "    keras.layers.Dense(units=16, activation=\"relu\"),\n",
        "    keras_uncertainty.layers.StochasticDropout(dropout_p),\n",
        "    keras.layers.Dense(units=16, activation=\"relu\"),\n",
        "    keras_uncertainty.layers.StochasticDropout(dropout_p),\n",
        "    keras.layers.Dense(units=1)\n",
        "])\n",
        "```\n",
        "In addition, you will also find there the predictions corresponding to the test set used in the previous lab (`mcdropout_regression_test_prediction.npy`)."
      ],
      "metadata": {
        "id": "ABGwK1d8KuVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_max = 3.14\n",
        "x_min_pos = 0.5\n",
        "n = 100\n",
        "assert n//2 != 0, f\"Only even number of data allowed (current {n})\"\n",
        "\n",
        "x_train_reg = np.concatenate([np.linspace(-x_max, -x_min_pos, num=n//2), np.linspace(x_min_pos, x_max, num=n//2)])\n",
        "y_test_reg = np.sin(x_train_reg)\n",
        "x_train_reg += np.random.normal(0, 0.1, x_train_reg.shape)\n",
        "\n",
        "x_test_max = 7\n",
        "n_test = 1000\n",
        "x_test_reg = np.linspace(-x_test_max, x_test_max, 1000)\n",
        "y_test_reg = np.sin(x_test_reg)\n",
        "# get predictions here"
      ],
      "metadata": {
        "id": "cchguibrVBlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB predictions are formulated as such by keras_uncertainty:\n",
        "\n",
        "$y_{pred} = (\\mu_{pred}, \\sigma_{pred})$"
      ],
      "metadata": {
        "id": "plb3DMmIrEby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def coverage(y_true:np.ndarray, y_pred:tuple(np.ndarray)):\n",
        "  # your code here\n",
        "  # confidence intervals\n",
        "  mu = y_pred[0]\n",
        "  sigma = y_pred[1]\n",
        "  ci_lower = mu - 2 * sigma\n",
        "  ci_upper = mu + 2 * sigma\n",
        "  within_interval = y_true > ci_lower and y_true < ci_upper\n",
        "  return (within_interval).mean()\n"
      ],
      "metadata": {
        "id": "35eB3Qwi37OY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "57f559c9-fcbd-4223-9230-cc18fce69abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'type' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2b99968136d7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcoverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;31m# your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# confidence intervals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'type' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Brier Score\n",
        "\n",
        "The Brier Score is an extension of the mean square error for probability distributions for classification. It requires the ground truth to be expressed as per-class probabilities. Hard labels need to be expressed using one-hot encodings. The corresponding predictions can be computed e.g. by _voting_.\n",
        "\n",
        "Summing up, by supposing a $C$-way classification problem and a test data point $x^{(i)}$ with corresponding ground truth $y^{(i)}$:\n",
        "\n",
        "* $y^{(i)} = \\begin{bmatrix} y_1^{(i)} & y_2^{(i)} & \\dots & y_C^{(i)} \\end{bmatrix}, y_c^{(i)} \\in [0,1]~\\forall c, \\sum_c y_c^{(i)} = 1$\n",
        "\n",
        "* $\\hat{y}^{(i)} = \\begin{bmatrix} \\hat{y}_{11}^{(i)} & \\dots & \\hat{y}_{1C}^{(i)}  \\\\ \\hat{y}_{21}^{(i)} & \\dots & \\hat{y}_{2C}^{(i)} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\hat{y}_{M1}^{(i)} & \\dots & \\hat{y}_{MC}^{(i)} \\end{bmatrix}, \\hat{y}_{jc}^{(i)} \\in [0,1]~\\forall j,c; \\sum_c \\hat{y}_{jc}^{(i)} = 1~\\forall j \\in {1,\\dots,M}$\n",
        "\n",
        "  We can summarize the output distribution by taking the mean across the various samples (components of the\n",
        "  ensemble, number of runs for VI...):\n",
        "\n",
        "  $$\n",
        "  \\bar{\\hat{y}}^{(i)} = \\begin{bmatrix}\\frac{\\sum_{i=1}^{M}\\hat{y}_{i1}^{(i)}}{M} & \\dots & \\frac{\\sum_{i=1}^{M}\\hat{y}_{iC}^{(i)}}{M} \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "We can then evaluate our model by having a sample of $N$ test data points, computing $\\bar{\\hat{y}}$ for each of them:\n",
        "\n",
        "$$\n",
        "\\text{BS}(y, \\hat{y}) = \\frac{\\sum_{i=1}^{N}\\sum_{c=1}^{C}\\left(y_{c}^{(i)}-\\bar{\\hat{y}}_{jc}^{(i)}\\right)^2}{NC}\n",
        "$$\n",
        "\n",
        "Note: if we have a binary classification problem, the formulation above simplifies, since $y$ is a 0-1 scalar and $\\hat{y}$ a vector of probabilities of assignment to the positive class."
      ],
      "metadata": {
        "id": "jGB6d8QP4GY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the implementation, we will need first to consider whether the outputs are passed as a simplex (i.e., a probability distribution) or as logits (i.e., a vector of real unbounded values). We will need to apply softmax to the logits in the latter case.\n",
        "\n",
        "**Your task**: recover one of yesterday's implementations of BNNs for MNIST classification and obtain the output from `StochasticClassifier` (or `EnsembleClassifier`) for the MNIST test set. Compute the brier score associated with the prediction."
      ],
      "metadata": {
        "id": "ImtNACB48zkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def brier_score(y_true, y_pred, preds_as_logits=False, axis_classes=2):\n",
        "  # your code here\n",
        "  pass\n",
        "\n"
      ],
      "metadata": {
        "id": "3O1iqTlNYrtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calibration\n",
        "\n",
        "Calibration is a measure of **trust** which concerns the relationship between accuracy and confidence.\n",
        "We expect accuracy that models have in their predictions to be high when the confidence assigned to these predictions is high; viceversa, low confidence should be related to low accuracy.\n",
        "The usage of accuracy, in this case, makes the concept of calibration quickly applicable to classification problems.\n",
        "\n",
        "We use the definition of confidence as the level of probability assigned to the predicted class (NB there are also other definitions, like entropy).\n",
        "If we consider the predictive posterior $P(y|x)$ as $\\bar{\\hat{y}}$ which we consideded before, then $\\text{confidence}(\\hat{y}) = \\max_{c\\in\\{1,\\dots,C\\}}\\left(\\bar{\\hat{y}}_c\\right)$. NB: we are talking about **max**, not **argmax** (which is related to the predicted class instead).\n"
      ],
      "metadata": {
        "id": "4Sf-IzT8Af9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Reliability plots\n",
        "\n",
        "We divide the 0-1 range for confidence in $K$ bins (e.g., $[0.0,0.1),[0.1,0.2),\\dots,[0.9,1.0]$). We call each of these bins $B_k, k\\in\\{1,\\dots,K\\}$.\n",
        "We assign each of the data points in our test dataset to these bins.\n",
        "We then retrieve the mean accuracy attained by the model on these points:\n",
        "$$\n",
        "\\text{Acc}(B_k) = \\sum_{i=1}^{N}{\\mathbb{1}_{\\left[i\\in B_k\\right]}\\frac{\\text{Acc}(y_i, \\hat{y}_i)}{N_k}},\n",
        "$$\n",
        "where $N_k$ indicates the number of elements in bin $B_k$.\n",
        "\n",
        "\n",
        "A reliability plot can be created as a plot with confidence on the x axis and accuracy on the y axis\n",
        "\n",
        "![](https://www.researchgate.net/profile/Anand-Avati/publication/321160854/figure/fig2/AS:562618466344960@1511150103061/Reliability-curve-calibration-plot-of-the-model-output-probabilities-on-the-test-set.png)\n",
        "\n",
        "A perfect calibration should happen when confidence = accuracy (black dotted line).\n",
        "\n",
        "\n",
        "**Your task:** below is a code for creating reliability plots using NumPy. You will notice that the functions for obtaining accuracy and confidence for probabilistic classifiers is incomplete. Complete these formula taking into account that the `predictions` are the output of a `StochasticClassifier`.\n",
        "Then, use the `reliability_vector` and the `reliability_plot` functions to plot the reliability plot associated with the data from the MNIST classification example.\n"
      ],
      "metadata": {
        "id": "o1rkD6KWhi26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_prob_models(predictions, ground_truth):\n",
        "  '''\n",
        "  NB be careful at how the ground_truth vector is passed; check if it is a\n",
        "  one-hot encoded vector or a list of ints.\n",
        "  In the first case, it is necessary to run also this through the argmax(1)\n",
        "  so we can recover a list of ints from it.\n",
        "  '''\n",
        "  return (predictions.argmax(1) == ground_truth).mean()\n",
        "\n",
        "def confidence_prob_models(predictions):\n",
        "  '''\n",
        "  In this case we consider predictions as the probability value of the\n",
        "  assignment class.\n",
        "  '''\n",
        "  return predictions.max(1)\n",
        "\n",
        "def confidence_binning(confidence_vector, n_bins=10):\n",
        "  '''\n",
        "  Returns the bin index associated to each datapoint and the bin composition\n",
        "  '''\n",
        "  bins = np.linspace(1/n_bins, 1, n_bins)\n",
        "  return np.digitize(confidence_vector, bins), bins\n",
        "\n",
        "def reliability_vector(predictions, ground_truth, n_bins=10):\n",
        "  '''\n",
        "  Given predictions and ground truth, calculates the confidence scores associated\n",
        "  with the predictions, bins the confidence into n_bins equispaced in the [0,1]\n",
        "  line, then compute the per-bin accuracy.\n",
        "  Returns a 1-d array of n_bins elements containing the per-bin accuracy, and an\n",
        "  array containing the cutoffs of each bin.\n",
        "  '''\n",
        "  confidence_scores = confidence_prob_models(predictions)\n",
        "\n",
        "  bins_composition, bins_cutoffs = confidence_binning(confidence_scores, n_bins)\n",
        "\n",
        "  mean_accuracy_per_bins = np.full((n_bins,), fill_value=np.nan)\n",
        "  bin_counts = np.bincount(bins_composition)\n",
        "\n",
        "  for i in range(n_bins):\n",
        "    if i > bins_composition.max():\n",
        "      break\n",
        "    if bin_counts[i] > 0:\n",
        "      group_accuracy = accuracy_prob_models(\n",
        "          predictions[bins_composition==i],\n",
        "          ground_truth[bins_composition==i]\n",
        "        )\n",
        "      mean_accuracy_per_bins[i] = group_accuracy\n",
        "\n",
        "  return mean_accuracy_per_bins, bins_cutoffs\n",
        "\n",
        "def reliability_plot(reliability_vector, bins_cutoffs, clear_nans=True):\n",
        "  bins_delta = bins_cutoffs[1] - bins_cutoffs[0]\n",
        "  x_axis = bins_cutoffs - bins_delta/2\n",
        "\n",
        "  if clear_nans:\n",
        "    x_axis = x_axis[~np.isnan(reliability_vector)]\n",
        "    reliability_vector = reliability_vector[~np.isnan(reliability_vector)]\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.scatter(\n",
        "      x_axis,\n",
        "      reliability_vector\n",
        "  )\n",
        "  ax.set_xlim((0,1))\n",
        "  ax.set_ylim((0,1))\n",
        "  line = mlines.Line2D([0, 1], [0, 1], color='red')\n",
        "  transform = ax.transAxes\n",
        "  line.set_transform(transform)\n",
        "  ax.add_line(line)\n",
        "  ax.set_xlabel(\"confidence\")\n",
        "  ax.set_ylabel(\"accuracy\")\n",
        "  plt.plot(x_axis, reliability_vector)\n",
        "  plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "3e2eR9Qod15E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and load the model for this part\n",
        "\n",
        "To see how the model was trained, refer to [this Colab notebook](https://colab.research.google.com/drive/1pP3ngtTIXcKb-e7XO2WEC4a-KEB6m3UZ?usp=sharing)"
      ],
      "metadata": {
        "id": "Cl0WgsUZ9num"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download_bayesian_model = True\n",
        "\n",
        "models_classification = {\n",
        "    \"frequentist\": {\n",
        "        \"url\": \"https://www.zullich.it/misc/fmnist_weights.keras\",\n",
        "        \"architecture\": lambda : keras.Sequential([\n",
        "                        keras.layers.Input(shape=(28, 28, 1)),\n",
        "                        keras.layers.Conv2D(filters=16, kernel_size=7, activation=\"relu\"),\n",
        "                        keras.layers.BatchNormalization(),\n",
        "                        keras.layers.Conv2D(filters=32, kernel_size=7, activation=\"relu\"),\n",
        "                        keras.layers.BatchNormalization(),\n",
        "                        keras.layers.Conv2D(filters=64, kernel_size=7, activation=\"relu\"),\n",
        "                        keras.layers.BatchNormalization(),\n",
        "                        keras.layers.Conv2D(filters=128, kernel_size=7, activation=\"relu\"),\n",
        "                        keras.layers.BatchNormalization(),\n",
        "                        keras.layers.GlobalAveragePooling2D(),\n",
        "                        keras.layers.Flatten(),\n",
        "                        keras.layers.Dense(10, activation=\"softmax\")\n",
        "        ])\n",
        "      },\n",
        "    \"bayesian\": {\n",
        "        \"url\": \"https://www.zullich.it/misc/fmnist_weights_b.keras\",\n",
        "        \"architecture\": lambda : keras.Sequential([\n",
        "                        keras.layers.Input(shape=(28, 28, 1)),\n",
        "                        keras.layers.Conv2D(filters=16, kernel_size=7, activation=\"relu\"),\n",
        "                        ku.layers.StochasticDropout(rate=0.1),\n",
        "                        keras.layers.BatchNormalization(),\n",
        "                        keras.layers.Conv2D(filters=32, kernel_size=7, activation=\"relu\"),\n",
        "                        ku.layers.StochasticDropout(rate=0.1),\n",
        "                        keras.layers.BatchNormalization(),\n",
        "                        keras.layers.Conv2D(filters=64, kernel_size=7, activation=\"relu\"),\n",
        "                        ku.layers.StochasticDropout(rate=0.1),\n",
        "                        keras.layers.BatchNormalization(),\n",
        "                        keras.layers.Conv2D(filters=128, kernel_size=7, activation=\"relu\"),\n",
        "                        ku.layers.StochasticDropout(rate=0.1),\n",
        "                        keras.layers.BatchNormalization(),\n",
        "                        keras.layers.GlobalAveragePooling2D(),\n",
        "                        keras.layers.Flatten(),\n",
        "                        keras.layers.Dense(10, activation=\"softmax\"),\n",
        "                    ])\n",
        "    }\n",
        "}\n",
        "model_metadata = models_classification[\"bayesian\" if download_bayesian_model else \"frequentist\"]\n",
        "\n",
        "download_weights(model_metadata[\"url\"], \"weights.keras\")\n",
        "\n",
        "\n",
        "\n",
        "model_fmnist = model_metadata[\"architecture\"]()\n",
        "model_fmnist.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model_fmnist.load_weights(\"weights.keras\")\n"
      ],
      "metadata": {
        "id": "SiBCqqszQzpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "x_test = np.expand_dims(x_test, axis=-1).astype(np.float32) / 255\n",
        "\n",
        "y_pred = model_fmnist.predict(x_test)\n"
      ],
      "metadata": {
        "id": "wKWjY7-AQ7Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc, bins_cutoffs = reliability_vector(y_pred, y_test, n_bins=10)"
      ],
      "metadata": {
        "id": "wcvaTWoozr6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use MNIST test results here\n",
        "reliability_plot(\n",
        "    acc, bins_cutoffs\n",
        ")\n"
      ],
      "metadata": {
        "id": "3ElkzhTVqFu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: _Is the above model underconfident or overconfident?_**"
      ],
      "metadata": {
        "id": "RxPerAP2cHGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calibration Error\n",
        "\n",
        "The calibration error can be easily computed out of the binning procedure carried out for obtaining the reliability plots\n",
        "\n",
        "$$\n",
        "\\text{CE}(y, \\hat{y}) = \\sum_{k=1}^{K} \\left\\vert \\text{Acc}(B_k) - \\text{Conf}(B_k) \\right\\vert\n",
        "$$"
      ],
      "metadata": {
        "id": "CBDe5F3ucVp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*As part of the final project, you are required to implement CE and ECE.*"
      ],
      "metadata": {
        "id": "VaYf8dEMww47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calibration_error(..., ...):\n",
        "  pass\n",
        "  #your code here"
      ],
      "metadata": {
        "id": "wTKGWYXEcGPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expected Calibration Error\n",
        "\n",
        "One of the issues with CE is that all bins are equally weighted. It would make more sense to have bins with more data be weighted more than others. ECE compensates for that:\n",
        "\n",
        "$$\n",
        "\\text{ECE}(y, \\hat{y}) = \\frac{\\sum_{k=1}^{K}N_k\\cdot \\left\\vert \\text{Acc}(B_k) - \\text{Conf}(B_k) \\right\\vert}{N}\n",
        "$$\n",
        "\n",
        "As defined before, $N_k$ is the number of elements belonging to bin $k$."
      ],
      "metadata": {
        "id": "BEbcaSgBfwur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expected_calibration_error(..., ...):\n",
        "  pass\n",
        "  #your code here"
      ],
      "metadata": {
        "id": "NGq749nCfwSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calibration for regression tasks\n",
        "\n",
        "While models for classification come with an inherent estimate of confidence, this is not true in the case of regression.\n",
        "Restricting to regression for 1-d targets, in probabilistic models we have a predictive posterior with a certain _precision_ (i.e., inverse of variance). We can intuitively think of a relationship whereas high precision $\\Leftrightarrow$ high confidence.\n",
        "\n",
        "Specifically, we can build **confidence intervals** for a given prediction. For this, we have to fix a **confidence level** $\\alpha$ and derive the corresponding interval around a given point, usually the expected value of the predictive posterior; in the case of a Gaussian, we build the interval around $\\mu$.\n",
        "\n",
        "![](https://d37djvu3ytnwxt.cloudfront.net/assets/courseware/v1/3c2230f2c98f3c61fcb4a4884ba96f84/asset-v1:DelftX+OT.1x+3T2016+type@asset+block/Normal_critical_values.png)\n",
        "Creation of a confidence interval for a Gaussian distribution (in the image denoted as $\\underline{z}$) around its expected value with three confidence levels $\\alpha=0.950, 0.990, 0.999$. _Image provided by TU Delft in the [\"Note on the interpretation of confidence interval\" course](https://ocw.tudelft.nl/course-readings/note-interpretation-confidence-interval/) under the CC-BY-SA License_.\n",
        "\n",
        "For a Gaussian predictive posterior $Z=P(Y|X)$ with mean $\\mu$, the task of determining the confidence interval boils down to determine the bounds $z_l$ and $z_u$ such as\n",
        "\n",
        "$$\n",
        "P(z_l \\leq \\mu \\leq z_u) = \\alpha\n",
        "$$\n",
        "\n",
        "In case the predictive posterior is a continuos Gaussian, the solution is $z_l = z_{\\frac{1-\\alpha}{2}} = \\Phi^{-1}\\left(\\frac{1-\\alpha}{2}\\right)\\cdot\\sigma + \\mu$; $\\Phi^{-1}$ indicating the inverse cumulative distribution function of the standard Gaussian. Due to the symmetry of the Gaussian distribution, $z_u$ can be quickly computed as $2\\mu - z_{l}$.\n",
        "\n",
        "In this way, a high-confidence interval will tend to be quite wide, while a low-confidence intervall will be thin around the expected value.\n",
        "\n",
        "Now that we have determined how to calculate the confidence intervals, we need to translate the concept of calibration to regression problem. The reliability plot for the regression problem can be built like this:\n",
        "\n",
        "Instead of doing confidence binning (like before), we:\n",
        "* Fix equispaced confidence values, e.g., $0.0, 0.1, \\dots ,0.9, 1.0$, call each of them $\\alpha_k, k=1,\\dots,K$. (NB: if we have a predictive posterior with unbounded support, we might wanna skip the value 1.0)\n",
        "* $\\forall \\alpha_k$:\n",
        "  * $\\forall x^{(i)} \\in X$ test dataset:\n",
        "    * Calculate the confidence interval for $\\hat{y}$ based on $\\alpha_k$\n",
        "    * Calculate the coverage, accuracy metric for regression, $c_k^{(i)} \\doteq \\text{Coverage}(y, \\hat{y})_{\\alpha_k}$\n",
        "    * Add $c_k^{(i)}$ to the plot\n",
        "\n",
        "Practical notes:\n",
        "* Using SciPy (or PyTorch, or Tensorflow) we can quickly compute the inverse CDF of a generic Gaussian. In SciPy, the command is `scipy.stats.normal(μ, σ).ppf(α)`, where μ, σ are the mean and std (NB, not the variance!) and α is the quantile. PPF stands for \"Percent Point Function\", which is another term for the inverse CDF.\n",
        "\n",
        "Statistical distributions in SciPy accept arrays as arguments, so `scipy.stats.normal([μ1, μ2], [σ1, σ2]).ppf(α)` will return an array of two scalars, the α-th quantile referred to the Gaussian with mean μ1 and std σ1 and the α-th quantile referred to the Gaussian with mean μ2 and std σ2.\n",
        "\n",
        "We can additionally get multiple quantiles by passing a column vector as α:\n",
        "we can define `α = np.array([α1,..., αk])` then call `scipy.stats.normal([μ1, μ2], [σ1, σ2]).ppf(α[:, np.newaxis])` to obtain all the α1,..., αk quantiles for the two Gaussians.\n",
        "NB: Don't skip the transposition into column vector (`α[:, np.newaxis]`)!\n"
      ],
      "metadata": {
        "id": "WXmbUTE5nPb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reliability_vector_regression(prediction, ground_truth, n_bins=10):\n",
        "  confidence_cutoffs = np.linspace(0, 1, n_bins+1)[:, np.newaxis] # already prepare the correct shape\n",
        "  quantiles = (1 - confidence_cutoffs) / 2\n",
        "\n",
        "  predictions_mean, predictions_std = prediction\n",
        "  interval_lower_bound = stats.norm(predictions_mean.squeeze(), predictions_std.squeeze()).ppf(quantiles)\n",
        "  interval_upper_bound = 2 * predictions_mean.squeeze() - interval_lower_bound\n",
        "\n",
        "  in_interval = (ground_truth >= interval_lower_bound) & (ground_truth <= interval_upper_bound)\n",
        "  coverage = in_interval.mean(1)\n",
        "\n",
        "  return coverage, confidence_cutoffs\n",
        "\n",
        "def reliability_plot_regression(reliability_vector, confidence_cutoffs):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.scatter(\n",
        "      confidence_cutoffs,\n",
        "      reliability_vector\n",
        "  )\n",
        "  ax.set_xlim((0,1))\n",
        "  ax.set_ylim((0,1))\n",
        "  line = mlines.Line2D([0, 1], [0, 1], color='red')\n",
        "  transform = ax.transAxes\n",
        "  line.set_transform(transform)\n",
        "  ax.add_line(line)\n",
        "  plt.plot(confidence_cutoffs, reliability_vector)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "-tbjHDo24C3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try it on the toy regression...\n",
        "\n",
        "Let us recreate the regression example and download the ensemble weights"
      ],
      "metadata": {
        "id": "ULHkvuV4FoFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def regression_gaussian_nll_loss(variance_tensor, epsilon=1e-8, variance_logits=False):\n",
        "    \"\"\"\n",
        "        Gaussian negative log-likelihood for regression, with variance estimated by the model.\n",
        "        This function returns a keras regression loss, given a symbolic tensor for the sigma square output of the model.\n",
        "        The training model should return the mean, while the testing/prediction model should return the mean and variance.\n",
        "    \"\"\"\n",
        "    def nll(y_true, y_pred):\n",
        "        #if variance_logits:\n",
        "        #    variance_tensor = K.exp(variance_tensor)\n",
        "\n",
        "        return 0.5 * K.mean(K.log(variance_tensor + epsilon) + K.square(y_true - y_pred) / (variance_tensor + epsilon))\n",
        "\n",
        "    return nll\n",
        "\n",
        "def ensemble_component_initializer():\n",
        "    inp = keras.Input(shape=(1,))\n",
        "    x = keras.layers.Dense(32, activation=\"relu\")(inp)\n",
        "    x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
        "    mean = keras.layers.Dense(1)(x)\n",
        "    var = keras.layers.Dense(1, activation=\"softplus\")(x)\n",
        "\n",
        "    train_model = keras.Model(inp, mean)\n",
        "    pred_model = keras.Model(inp, [mean, var])\n",
        "\n",
        "    train_model.compile(loss=regression_gaussian_nll_loss(var), optimizer=\"adam\")\n",
        "\n",
        "    return train_model, pred_model\n",
        "\n",
        "n_components = 10\n",
        "n_epochs = 500\n",
        "\n",
        "ensemble = ku.models.DeepEnsembleRegressor(ensemble_component_initializer, num_estimators=n_components)"
      ],
      "metadata": {
        "id": "DNn-4lk8-40f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_weights(\"https://www.zullich.it/misc/ensemble.keras.zip\", \"ensemble.keras.zip\")\n",
        "\n",
        "!unzip ensemble.keras.zip\n",
        "\n",
        "ensemble.load_weights(\"ensemble.keras\")"
      ],
      "metadata": {
        "id": "JgIT53C8DQiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_ensemble = ensemble.predict(x_test_reg)"
      ],
      "metadata": {
        "id": "0m-hbzHHD7Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rv, cc = reliability_vector_regression(y_pred_ensemble, y_test_reg)\n",
        "reliability_plot_regression(rv, cc)"
      ],
      "metadata": {
        "id": "5WC4JtASQgDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final task**: let us subset the predictions and the ground truth to only the units in the train set lying in the interval [-3.14, 3.14]. This should correspond (roughly) to the distribution of the training data. Does something change now?"
      ],
      "metadata": {
        "id": "qreFz5SIG1ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.bitwise_and(x_test_reg>-3.14,x_test_reg<3.14)"
      ],
      "metadata": {
        "id": "bnYRYy_BFuOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_reg_masked = x_test_reg[mask]\n",
        "y_pred_ensemble_masked = (y_pred_ensemble[0][mask],y_pred_ensemble[1][mask])"
      ],
      "metadata": {
        "id": "LH8nFPaC68yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_reg_masked = y_test_reg[mask]"
      ],
      "metadata": {
        "id": "tTvXFjxJ7YNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rv, cc = reliability_vector_regression(y_pred_ensemble_masked, y_test_reg_masked)\n",
        "reliability_plot_regression(rv, cc)"
      ],
      "metadata": {
        "id": "zEFGo4U77PvY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}