{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Wu8jxnrteM_D"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","from matplotlib import pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"A7U7jcESezVM"},"source":["## construct a simple dataset for a simple problem\n","\n","We will fit a sinusoid from a limited number of samples.\n","\n","The nature of this problem is perfect for this course: we want to accompany predictions with a level of uncertainty. The uncertainty should be low as we get close to the training points, high otherwise.\n","\n","We create two linear sequences of 15 points with a \"hole\" in the middle. This will serve as a training data for our machine learning model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"id":"Z1q-CumGed40","outputId":"e20bad7c-6c8b-4b86-c09d-76e16763d388"},"outputs":[],"source":["x_max = 3.14\n","x_min_pos = 0.5\n","n = 30\n","assert n//2 != 0, f\"Only even number of data allowed (current {n})\"\n","\n","x = np.concatenate([np.linspace(-x_max, -x_min_pos, num=n//2), np.linspace(x_min_pos, x_max, num=n//2)])\n","y = np.sin(x)\n","x += np.random.normal(0, 0.1, x.shape)\n","\n","\n","\n","# plot the data\n","plt.scatter(x, y)"]},{"cell_type":"markdown","metadata":{"id":"aC0Cgikcqek1"},"source":["This dataset could be fit pretty easily by a plethora of very simple machine learning models; however, in this course we are mainly focused on neural networks (NNs).\n","\n","Specifically, we will quickly review how to train a very small NN using Keras as a refresher of this library."]},{"cell_type":"markdown","metadata":{"id":"RKM8yXth5XEG"},"source":["### Detour on tensors\n","\n","Tensorflow (and PyTorch) revolve around the concept of tensor.\n","For our purposes, we can view the tensor as an extension of the concept of vector and matrix into $n$ dimensions.\n","\n","A vector of size $k_1$ can be seen as an ordered collection of $k_1$ scalars. We represent it as a vertical stack.\n","\n","![](https://drive.google.com/uc?export=view&id=1o2RiXi7fC3E-GBkL_rLxuFVi6OV1nrqV)\n","\n","A vector is ideal for representing lists of homogeneous data: for instance, we can think of having a reading of a sensor for $k_1$ time steps.\n","\n","The matrix adds another dimension of size $k_2$. We can represent it by repeating the previous vector of size $k_1$ for $k_2$ times, then stack these slices horizontally, thus obtaining a grid of size $k_1$ times $k_2$:\n","\n","![](https://drive.google.com/uc?export=view&id=1334LpKUNNS5JjGdPzvNzAUFlwFBMDA63)\n","\n","By extending the previous sensor example, we can have $k_2$ different sensors, each with its $k_1$ readings. We can store these data into a matrix.\n","\n","We can extend the matrix to a generic 3-dimensional structure by doing as we did with the extension from vector to matrix: just repeat the matrix structure $k_3$ times and stack them together.\n","\n","![](https://drive.google.com/uc?export=view&id=1QGeBKzjrcgk6zl14qoPrldNlQMtOFfaI)\n","\n","We have now obtained a 3-d **tensor** of dimension $k_1 \\times k_2 \\times k_3$. Recalling the sensor example, we can now extend this example to fit into a 3-d tensor.\n","Let us suppose that our company has several facilities (specifically $k_3$ of them), each facility having $k_2$ tensors recording $k_1$ readings in a defined time frame. We can store these readings separately in each matrix, and stack them together into a 3-d tensor.\n","Generically, a 3-d tensor can also be seen as a **list of matrices** all having the **same shape**.\n","\n","![](https://drive.google.com/uc?export=view&id=1bLQXRB4bqzf3LhbD4_ihsXqp3r7xc_ku)\n","\n","The extension to a 4-d tensor is then simple: a list of 3-d tensor of the same shape, or a list of list of matrices.\n","Notice that **all sub-structures have the same shape**.\n","\n","![](https://drive.google.com/uc?export=view&id=176sAfZ4UOrpytY_5DxoVxY52UFxmwyrm)\n"]},{"cell_type":"markdown","metadata":{"id":"2VkGWM8Hnjmn"},"source":["#### Tensors in Python\n","\n","There exist many libraries introducing the concept of tensors or $n$-dimensional arrays in Python, the most famous one being `numpy`, where they are called `ndarray`s.\n","We have previously defined `x` as a one-dimensional array.\n","Next, we defined `y` by applying `np.sin`, a numpy function, to `x`. This function is applied element-wise to `x`, and thus produces a new one-dimensional array of the same shape as `x`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eJPWutlQnfMY","outputId":"371a96d1-3ac0-4285-f66a-68cfe97f7191"},"outputs":[],"source":["print(\"x\")\n","print(x, \"\\n\")\n","\n","print(\"y\")\n","print(y, \"\\n\")\n","\n","print(\"Shape of x:\", x.shape)\n","print(\"Shape of y:\", y.shape)"]},{"cell_type":"markdown","metadata":{"id":"9PDaZHbOoNFf"},"source":["We can immediately notice that we have used no explicit loops to compute `y`. If we used lists to store the values of `x`, we would have needed to write the following:\n","```python\n","import math\n","\n","y = []\n","for element in x:\n","  y = math.sin(x)\n","```\n","\n","or, using list comprehensions:\n","\n","```python\n","import math\n","\n","y = [math.sin(element) for element in x]\n","```\n","\n","These code pieces force the construction of `y` element-by-element, in a sequential way.\n","Instead, by using tensors and function for tensors, we are implicitly using all sorts of vectorized computations which exploit parallelism in the CPU (or, as we will later see, in GPUs) to get much faster results.\n","\n","On to $n$-dimensional arrays, we can construct them quickly in NumPy using one of four functions:\n","\n","```python\n","shape = (2,3,5,4) # a list or tuple containing the shape of the tensor\n","np.zeros(shape) # -> creates a tensor of zeros of the desired shape\n","np.ones(shape) #  -> creates a tensor of ones of the desired shape\n","np.empty(shape) # -> creates an empty tensor of the desired shape\n","                #    Note that the content is absolutely garbage!\n","np.full(shape, fill_value=np.pi) # -> creates a tensor with the desided shape, all elements will have the specified fill_value\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bEDVSRCgqH8e","outputId":"de2b69c8-de67-49a4-df3d-d75e6db89ec4"},"outputs":[],"source":["full_tensor = np.full((2, 3, 5, 4), np.pi)\n","\n","print(full_tensor)\n","print(\"Shape:\", full_tensor.shape)"]},{"cell_type":"markdown","metadata":{"id":"k3zzLxERrtYi"},"source":["We can additionally use functions to create random tensors, like\n","`np.random.rand` (for sampling uniformly the elements of the tensors between 0 and 1) or `np.random.randn` (for sampling elements from a standard Gaussian distribution). See the [documentation for NumPy random generation](https://numpy.org/doc/stable/reference/random/legacy.html) for additional info."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1sOo-EMFrtHg","outputId":"4bb1f600-291b-4f3d-c393-1cc1a5fe6ab3"},"outputs":[],"source":["np.random.randn(3,3,4)"]},{"cell_type":"markdown","metadata":{"id":"q2LVYs7gsUiI"},"source":["Before going on to Keras, it's worth to concentrate on the tensors data types and operations.\n","\n","Conversely to lists, tensors and ndarrays can hold data of the same type. That means only bools, ints, floats, etc. of the same precision. For instance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yUU6ZVJ1s8rp","outputId":"8b934bed-043b-4554-aa11-800712ef96fd"},"outputs":[],"source":["x.dtype"]},{"cell_type":"markdown","metadata":{"id":"CDh9ug2Ls_Rj"},"source":["We can change the precision of an ndarray by using the function `astype`:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kC2lS13Ss-6e","outputId":"b432525e-92a4-4e12-a793-628906242231"},"outputs":[],"source":["rand_ndarray = np.random.randn(3, 4)\n","\n","print(rand_ndarray)\n","print(rand_ndarray.dtype)\n","\n","rand_ndarray_float16 = rand_ndarray.astype(np.float16) # same as rand_ndarray.astype(\"float16\")\n","\n","print(rand_ndarray_float16)\n","print(rand_ndarray_float16.dtype)\n","\n","# Be careful at possible loss of information!\n","\n","rand_ndarray_int8 = rand_ndarray.astype(np.int8) # same as rand_ndarray.astype(\"int8\")\n","\n","print(rand_ndarray_int8)\n","print(rand_ndarray_int8.dtype)"]},{"cell_type":"markdown","metadata":{"id":"BnghGJuXuBYE"},"source":["We can create subsets of tensors by means of **slicing operators**, which append a set of square brackets `[]` at the end of the tensor name.\n","\n","This allows us to select one or more element of the tensors, at any position within the hierarchy of dimensions.\n","\n","`x[0]` select the first element of the first dimension of the tensor `x`. In the case of a 4-d tensor interpretable as a list of 3-d tensors, this will select the first 3-d tensor in this list:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9QCS-YiZutzq","outputId":"3d6d18e9-4767-4709-bbfa-410498b5990e"},"outputs":[],"source":["tensor_4d = np.random.randint(0, 10, (2,3,3,4))\n","\n","print(\"tensor_4d:\\n\", tensor_4d)\n","print(\"shape:\", tensor_4d.shape, \"\\n\")\n","\n","print(\"tensor_4d [0]:\\n\", tensor_4d[0])\n","print(\"shape:\", tensor_4d[0].shape, \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"wuOytBM_vNmF"},"source":["If we want to select a single scalar in our tensor, we have to specify a value for all the four dimensions:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LCB9LFnQvNNM","outputId":"62063f88-3b88-48a0-b729-2956375b8cc4"},"outputs":[],"source":["tensor_4d[0, 1, 1, 3]"]},{"cell_type":"markdown","metadata":{"id":"n-IYeI-swIUt"},"source":["If we want to select a single structure in the second dimension of the hierarchy, we have to indicate a `:` in the place corresponding to the first:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7IrkPGkZwH9p","outputId":"562bf14b-b322-4f79-e615-36a7e427d43f"},"outputs":[],"source":["tensor_4d[:, 1] # this returns, for all the 3-d tensors in our list, only the second matrix"]},{"cell_type":"markdown","metadata":{"id":"brcQpvUowuDh"},"source":["We can also select multiple elements in the slices. For instance, we can use intervals. For instance, if we want the last two columns of the last matrix of the last 3-d tensor in the list, we can type"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dYXGyLDrxFLC","outputId":"195ba1bb-d01f-41ed-9dbd-8e37cade3b82"},"outputs":[],"source":["tensor_4d[1, 2, :, 2:4] # 2:4 = select rows from 2 (included) to 4 (not included)"]},{"cell_type":"markdown","metadata":{"id":"JN8t4a56xbOI"},"source":["For the same task we can also use negative indices in the slice. Negative indices start from the last element (indexed as `-1` and decrease accordingly). `x[-5]` returns the fifth-to-last element of tensor `x`:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"URtgXY5pxrDb","outputId":"0d0b0df5-60a7-4620-a18e-3984fbddb82b"},"outputs":[],"source":["tensor_4d[-1, -1, :, 2:4]"]},{"cell_type":"markdown","metadata":{"id":"Kfupx-oUx8ej"},"source":["We can also omit the `4` in the last position of the slice: this means \"pick all elements from 2 to the end\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A10pr18vx8KF","outputId":"1a5d47f6-bc3f-4479-8b8a-3b6a227ee20e"},"outputs":[],"source":["print(tensor_4d[-1, -1, :, 2:])\n","\n","# alternatively, using negative indices\n","\n","print(tensor_4d[-1, -1, :, -2:])"]},{"cell_type":"markdown","metadata":{"id":"N2ixFiyWyOsJ"},"source":["Finally, we may also use lists or tuples in slices to select specifically multiple elements. For instance, if we want the first and last row of each of the matrices contained in our 4d tensor, we will type"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yk6ZXilaydWN","outputId":"51abb129-cc82-49a7-b46e-10b38e0ba92d"},"outputs":[],"source":["tensor_4d[:, :, (0, -1), :] # alternatively, tensor_4d[:, :, (0, -1)]"]},{"cell_type":"markdown","metadata":{"id":"HL3RKlLmxEvr"},"source":[]},{"cell_type":"markdown","metadata":{"id":"j8fTgK4VtvyP"},"source":["On to tensor/ndarray operations:\n","\n","A sum/subtraction/multiplication/division between a tensor and a scalar will always be interpreted as the same operation between the same scalar and each element of the tensor:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AhUa3z3vtvLY","outputId":"a0a94850-900d-4ef2-c2fd-2b965ea02d27"},"outputs":[],"source":["print(\"x:\")\n","print(x)\n","\n","print(\"2*x:\")\n","print(2*x)\n","\n","print(\"x//2: (integer division)\")\n","print(x//2)"]},{"cell_type":"markdown","metadata":{"id":"teyT5kw-07WX"},"source":["We can also apply an operation between tensors when these tensors are **conformable** (same shape). This operation will **always** be interpreted as an element-by-element operation. Be very careful when you want to, e.g., multiply matrices in this way."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dKzxot_i1C5Y","outputId":"d9c6dbbb-aa8e-45cc-a853-bb47ef80d531"},"outputs":[],"source":["z = np.full_like(x, 2) # matrix of 2's of the same shape as x\n","\n","print(\"x * z: (equivalent to x * 2)\")\n","print(x * z)"]},{"cell_type":"markdown","metadata":{"id":"UjrYYUNw1hQi"},"source":["The usual matrix multiplication has to be computed differently.\n","Let us show an example.\n","\n","$a$ is a $5 \\times 4$ matrix, $b$ is a $4 \\times 5$ matrix.\n","We could do $a\\cdot b$ to obtain a $5\\times 5$ matrix as output:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"EIPFQUE81o2Q","outputId":"78abbbcd-e22f-451b-9677-433a47c9ac1e"},"outputs":[],"source":["a = np.random.rand(5, 4)\n","print(\"a:\\n\", a)\n","b = np.random.rand(4, 5)\n","print(\"b:\\n\", b)\n","a*b"]},{"cell_type":"markdown","metadata":{"id":"LugYHjaz2DcG"},"source":["We, instead, have to use `np.dot(a, b)`, which computes the actual multiplication:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rTVdfRN81_26","outputId":"a2ac1a39-51a4-458d-9eba-cb21196dcf78"},"outputs":[],"source":["np.dot(a, b)"]},{"cell_type":"markdown","metadata":{"id":"UlXBbRmf2ZoS"},"source":["We could also do the same with the transposed matrices. We can transpose the two matrices by typing `a.T` and `b.T`. In this case, their dot product will yield a $4\\times 4$ matrix:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HYK0hhZY2iZw","outputId":"d8de084b-9423-4bb5-ef6a-09ae66c12a54"},"outputs":[],"source":["np.dot(a.T, b.T)"]},{"cell_type":"markdown","metadata":{"id":"HaHQ88b22uii"},"source":["The operations between tensors also use the concept of **broadcasting**: it is a complex concept that means that NumPy will try and compute the operation even when there is a partial match between the dimensions of two tensors."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BrqXnWWT28xV","outputId":"c2658864-d147-4ef9-e784-aabc06b62021"},"outputs":[],"source":["one_row = np.array([[1,2,3,4]])\n","print(\"row vector:\\n\", one_row, \"\\n\\n\")\n","print(\"row vector (shape 1 x 4) * conformable matrix (shape 5 x 4):\\n\", a * one_row)"]},{"cell_type":"markdown","metadata":{"id":"GUkJT6Hl3mKu"},"source":["In the above code, we have one row vector, which we have defined by passing a list of a list of five elements to the constructor `np.array`.\n","In order to muliply this row with a matrix of conformable size, we need three conditions to happen:\n","1. The number of dimensions of these tensors have to be the same (so, 2)\n","2. The two tensors must share the exact shape on a subset of their dimensions, at the same indices (so, for instance, dimensions 1, 2, and 5 must match)\n","3. The other dimensions of one of the two tensors is a _singleton dimension_ (i.e., exactly 1)\n","\n","In the above case, we have one operand with shape 1 x 4 and another one with shape 5 x 4. The three conditions above are all satisfied, so the operation is carried out.\n","\n","The result is that all of the rows of the matrix `a` are multiplied, element-by-element, by the same copy of the row vector above defined.\n","If you notice, the first column is the same as the first column of `a`, the second is multiplied by two, etc.\n","\n","If we want to obtain the same result with a column vector, we just need to add one transposition while defining this vector, and the job is done:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wjYvvjnh3ksF","outputId":"17801e31-42d2-4336-e153-f2398b9b9ff7"},"outputs":[],"source":["one_column = np.array([[1,2,3,4,5]]).T\n","print(\"column vector:\\n\", one_column, \"\\n\\n\")\n","print(\"column vector (shape 5 x 1) * conformable matrix (shape 5 x 4):\\n\", a * one_column)"]},{"cell_type":"markdown","metadata":{"id":"8RZR6PNc5txo"},"source":["#### Transitioning from NumPy to Keras\n","\n","Keras is a library built on top of TensorFlow (TF) to facilitate the development of neural network-based models.\n","While\n","**Tensor**flow (TF) is a library for scientific computing, specifically revolving around tensor computations, as the name suggests.\n","\n","The code for creating and manipulating tensors in TF is very similar to NumPy, although TF tensors have a natural support for GPU computations and are built in such a way to support automatic differentiation using backpropagation. For the sake of this series of labs, we will not go more into the details concerning the differences, as Keras allows also to use NumPy arrays as data for training and evaluating their models."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLMs8Z5b6tlR","outputId":"a710e519-6c0c-4f0f-a307-41947f6c1d07"},"outputs":[],"source":["tf.constant([[1,2,3],[4,5,6]])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9imdOrgY7H2c","outputId":"7654e8e2-0b20-4419-a2a2-81ae3436e7aa"},"outputs":[],"source":["tf.random.uniform((2,3)) # as np.random.rand"]},{"cell_type":"markdown","metadata":{"id":"7MEoIwzWqxtD"},"source":["## Training a NN with Keras\n","\n","NNs are **frequentist** machine learning models loosely inspired by their biological counterpart: the data enters the NN and passes through some intermediate steps of evaluation, usually encompassing:\n","* a matrix-matrix (or vector-matrix) product between data and weights\n","* a non-linear function applied element-wise to each of the outputs of the previous product, which forms a **latent** or **hidden layer** in the computation.\n","  * the dimension of this latent space is fixed\n","\n","The output of the final computation of this sequence of steps is the output of our NN.\n","\n","The process is more easily represented by means of a computational graph, where we put the input on the left. We draw a **node** for each dimension of the input and stack the nodes vertically.\n","We do the same with all the dimensions of the hidden layers and the output as well.\n","The output is the rightmost point of this graph.\n","\n","In the simplest form of NN, the MultiLayer Perceptron (MLP), we have full connectivity between each node of each consecutive layer.\n","This means that\n","\n","![](https://raw.githubusercontent.com/ansuini/DSSC_DL_2022/71a0768e2c9370a3e1fff29cddbf45f386df4410/labs/imgs/01/mlp_graph.jpg)"]},{"cell_type":"markdown","metadata":{"id":"DVqM583oyfiP"},"source":["An MLP can be quickly built in Keras using simple building blocks found in the submodule `keras.layers`.\n","\n","If the information if the NN flows sequentially (i.e., we compute the content of each hidden layer sequentially after the previous layer has been evaluated), we can encapsulate our NN using the very convenient `keras.models.Sequential` structure.\n","\n","The `Sequential` structure is constructed from a list of layers, like so"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eq_NQJuhyerI","outputId":"a4f483fc-7321-4e14-ac6a-fd36a6acbf4d"},"outputs":[],"source":["model = keras.models.Sequential([\n","    keras.Input(shape=(1,)),\n","    layers.Dense(units=16, activation=\"relu\"),\n","    layers.Dense(units=16, activation=\"relu\"),\n","    layers.Dense(units=1)\n","])\n","\n","print(model.output_shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8khH94g2oaF"},"outputs":[],"source":["model.compile(\n","    optimizer=\"Adam\",\n","    loss=\"mean_squared_error\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nFDvjEBiPpaF","outputId":"c9a4d191-2342-4ed8-8b71-f4906bc0736b"},"outputs":[],"source":["model.fit(\n","    x, y, epochs = 2000, verbose=1\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":465},"id":"TELUd0VL0Ryn","outputId":"23b26ea7-65c4-498f-9003-417cc5115b26"},"outputs":[],"source":["y_pred = model.predict(x)\n","\n","plt.scatter(x, y, s=5)\n","plt.scatter(x, y_pred, s=5)"]},{"cell_type":"markdown","metadata":{"id":"aotcRHDP7WC_"},"source":["#### Testing our model\n","\n","We can now test our model. We randomly sample a set of `m` points and proceed to test the effectiveness of our model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pafM_UO07mbU"},"outputs":[],"source":["x_test = np.linspace(-3.14, 3.14, 1000)\n","y_test_true = np.sin(x_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w1zwUdgA79Ut","outputId":"79e2b5d0-dcf0-4874-d350-467f579f810f"},"outputs":[],"source":["y_test_pred = model.predict(x_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_5rkoC6r8cDg","outputId":"1bf88cca-7f30-4def-bf66-708d2fd83d4e"},"outputs":[],"source":["keras.losses.MeanSquaredError()(y_test_pred, y_test_true)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8sdjmpKT8mKf"},"outputs":[],"source":["def plot_prediction(x, y_true, y_pred, x_max, x_min_pos):\n","  plt.scatter(x, y_true, s=5)\n","  plt.scatter(x, y_pred, s=5)\n","  yline = (\n","      min(y_true.min().item(), y_pred.min().item()),\n","      max(y_true.max().item(), y_pred.max().item())\n","  )\n","  plt.vlines((-x_max, -x_min_pos, x_min_pos, x_max), ymin=yline[0], ymax=yline[1], colors=\"black\")\n","  x1 = np.array((-x_max, -x_min_pos))\n","  x2 = np.array((x_min_pos, x_max))\n","  y1 = np.array([yline[1], yline[1]])\n","  y2 = np.array([yline[0], yline[0]])\n","  plt.fill_between(x1, y1, y2, where=(y1 > y2), color=\"yellow\", alpha=.3)\n","  plt.fill_between(x2, y1, y2, where=(y1 > y2), color=\"yellow\", alpha=.3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"IahmYeb_2xcj","outputId":"f9383322-2ae2-4847-d15f-d22c0d6408ae"},"outputs":[],"source":["plot_prediction(x_test, y_test_true, y_test_pred, x_max, x_min_pos)"]},{"cell_type":"markdown","metadata":{"id":"ACMEHykSIRUK"},"source":["**Q: _comment on this chart? Do you know why are those rough edges present?_**"]},{"cell_type":"markdown","metadata":{"id":"VQMSWQ0a3AcP"},"source":["#### Testing outside of the range $[-\\pi,\\pi]$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tp-hXblQ_yY9","outputId":"f08f567a-cd63-4779-82e1-10aa6d737559"},"outputs":[],"source":["x_test_out = np.concatenate([np.linspace(-7, -x_max, 1000), np.linspace(x_max, 7, 1000)])\n","y_test_out_true = np.sin(x_test_out)\n","y_test_out_pred = model.predict(x_test_out)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"tyyEPMUh2aqb","outputId":"659c80d8-54ce-46bc-c712-545093914b85"},"outputs":[],"source":["plot_prediction(x_test_out, y_test_out_true, y_test_out_pred, x_max, x_min_pos)"]},{"cell_type":"markdown","metadata":{"id":"0jd9hzB0Ic-H"},"source":["**Q: _why are the predictions so seemingly random outside of the range $[-\\pi, \\pi]$?_**"]},{"cell_type":"markdown","metadata":{"id":"zYRnMZ0T3Fvn"},"source":["#### Combining everything\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"xAAzmv5f3IL2","outputId":"2ec56026-5925-4fd5-8c47-0c7831b0c068"},"outputs":[],"source":["x_test_all = np.concatenate([x_test, x_test_out])\n","y_test_all_true = np.concatenate([y_test_true, y_test_out_true])\n","y_test_all_pred = np.concatenate([y_test_pred, y_test_out_pred])\n","plot_prediction(x_test_all, y_test_all_true, y_test_all_pred, x_max, x_min_pos)"]},{"cell_type":"markdown","metadata":{"id":"QxONi1jCNvg-"},"source":["### What about the uncertainty\n","\n","**Q**: Can we get any information on the uncertainty for this model?\n","\n","Unfortunately, no, this model only outputs a scalar value, hence we have no information on uncertainty.\n","\n","We would like to have a way to identify whether the orange predictions are certain/uncertain (i.e., which is the variance of the predictions).\n","This is only possible with the UE methods we will see later.\n","\n","#### What about classification NNs?\n","\n","Let's review how to build and train a model on MNIST..."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":802,"status":"ok","timestamp":1706197089412,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"HVLmSbraRGiA","outputId":"26fd9c04-f7d3-49a8-8abc-0dd301b97ab7"},"outputs":[],"source":["model_mnist = keras.Sequential([\n","    keras.layers.Input((28, 28, 1)), # data as image: heigth x width x channels. MNIST is grayscale Â» 1 channel\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(32, activation=\"relu\"),\n","    keras.layers.Dense(10),\n","    keras.layers.Activation(\"softmax\")\n","])\n","\n","model_mnist.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) # notice the different loss function & metric\n","\n","model_mnist.summary()"]},{"cell_type":"markdown","metadata":{"id":"exOiFRdoRgjy"},"source":["Download and preprocess the data..."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2409,"status":"ok","timestamp":1706197107911,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"eI5TLstWRPfB","outputId":"4c838b22-2fe9-4a04-fe92-ac19f1a19ad1"},"outputs":[],"source":["(x, y), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n","\n","def preprocess_data(data):\n","  '''\n","  Changes data type to float32, normalizes in [0,1] range, and add a channel singleton dimension\n","  '''\n","  return np.expand_dims(data.astype(\"float32\"), -1) / 255\n","\n","x = preprocess_data(x)\n","x_test = preprocess_data(x_test)\n","\n","y = keras.utils.to_categorical(y, 10)\n","y_test = keras.utils.to_categorical(y_test, 10)"]},{"cell_type":"markdown","metadata":{"id":"KMelerpeRigC"},"source":["Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26437,"status":"ok","timestamp":1706197220308,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"KRjsqY5lRiH7","outputId":"01943484-05d4-4b79-bfa6-ef570a58805a"},"outputs":[],"source":["model_mnist.fit(x, y, batch_size=128, epochs=15)"]},{"cell_type":"markdown","metadata":{"id":"RiPjcszBR0q7"},"source":["Testing..."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2140,"status":"ok","timestamp":1706197244231,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"qpMiwQ7hR11U","outputId":"8a697675-23ce-4c72-ce27-579dbf4d3109"},"outputs":[],"source":["predictions = model_mnist.predict(x_test)"]},{"cell_type":"markdown","metadata":{"id":"ytKOeiOsTORZ"},"source":["We want to calculate accuracy. How are predictions and ground truth encoded?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706197567631,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"cIxipKARTB4Z","outputId":"ce8e7186-0db1-4615-e0e1-c51b1cd0821e"},"outputs":[],"source":["print(predictions, \"\\n\")\n","\n","print(y_test)"]},{"cell_type":"markdown","metadata":{"id":"UbkLk29WTStY"},"source":["To use accuracy, we need to convert the predictions and accuracy to labels.\n","We can do it by assigning each prediction to the index of the maximum value.\n","\n","This is done using `argmax`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234,"status":"ok","timestamp":1706197685750,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"H0So436SR-U8","outputId":"12f03d51-f63d-4da2-9070-3028ac0d3371"},"outputs":[],"source":["pred_labels = predictions.argmax(axis=1)\n","test_labels = y_test.argmax(axis=1)\n","\n","metric = keras.metrics.Accuracy()\n","metric.update_state(pred_labels, test_labels)\n","metric.result()"]},{"cell_type":"markdown","metadata":{"id":"ZWsCfcECTq1a"},"source":["Let's roll back to the predictions. Can we actually get an idea on uncertainty?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706197717017,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"6eat4erVTqbZ","outputId":"391112ed-ff23-4450-95aa-3bb523e61dfc"},"outputs":[],"source":["predictions[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"executionInfo":{"elapsed":585,"status":"ok","timestamp":1706198137187,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"wsw6N8iGTxzJ","outputId":"6bb31f66-e7a5-4098-894c-fc7aa6e7a02f"},"outputs":[],"source":["def plot_predictions(prediction, label=None):\n","  _=plt.bar(x=np.arange(0,10), height=prediction.squeeze())\n","  _=plt.xticks(np.arange(0,10))\n","  class_prediction = prediction.argmax()\n","  plt.ylim((0,1))\n","\n","  title = f\"Predicted class: {class_prediction}\"\n","  if label is not None:\n","    title += f\"- Correct label: {label}\"\n","\n","  _ = plt.title(title)\n","\n","  plt.show()\n","\n","plot_predictions(predictions[0], label=test_labels[0])"]},{"cell_type":"markdown","metadata":{"id":"RnlyySF9UGtz"},"source":["This vector can be interpreted as a probability distribution.\n","\n","We can interpret the confidence of the model as the $\\max$ of this vector.\n","\n","Let's see an example in which the model is misclassifying the data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":251},"executionInfo":{"elapsed":6,"status":"error","timestamp":1706198913404,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"-SQTSaDBUaCq","outputId":"00978cbb-bc83-4bcb-8cdf-6d6929cf7fea"},"outputs":[],"source":["misclassifications = pred_labels != test_labels\n","index = misclassifications.nonzero()[0][0]\n","\n","\n","plot_predictions(\n","    predictions[index],\n","    label=test_labels[index]\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.imshow(x_test[index], cmap=\"gray\")"]},{"cell_type":"markdown","metadata":{},"source":["**Q**: *How high is the confidence in this case?*\n","\n","\n","Deep Neural Networks can easily classify very confidently random images, as noticed in the paper [**Deep neural networks are easily fooled: High confidence predictions for unrecognizable images**](https://ieeexplore.ieee.org/document/7298640) by Nguyen et al., 2015.\n","\n","![](https://www.researchgate.net/profile/Anh-Nguyen-282/publication/307560165/figure/fig1/AS:476566221266944@1490633649679/Evolved-images-that-are-unrecognizable-to-humans-but-that-state-of-the-art-DNNs-trained.png)\n","\n","The image above, which is part of the same paper, the authors show how completely random images can be classified by the models employed as existing categories with confidence > 99%."]},{"cell_type":"markdown","metadata":{"id":"pqPYoZqZIGnu"},"source":["This introductory notebook was just a preview of things to come. Let's start working with BNNs next!"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPhviD0TuiJrsl5lL6MmVAH","provenance":[{"file_id":"https://github.com/marcozullich/UEML-AIDA-24/blob/master/00_tensors_keras_refresher.ipynb","timestamp":1706198949319}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
