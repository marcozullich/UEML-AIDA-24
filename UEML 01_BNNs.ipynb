{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvaldenegro/UncertaintyML-course-ESSAI-labs/blob/main/01_BNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_Z2tZ_tCSqZ",
        "outputId": "13108cc1-e6fe-4f44-a176-24cd57492126"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/mvaldenegro/keras-uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2QMJSnu3ilr"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import keras_uncertainty as ku\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# THIS TO RUN FUNCTIONAL KERAS MODELS\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "disable_eager_execution()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yURVFVat8RID"
      },
      "source": [
        "Re-create the dataset from the intro..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8-oXo_rEUH9"
      },
      "outputs": [],
      "source": [
        "x_max = 3.14\n",
        "x_min_pos = 0.5\n",
        "n = 100\n",
        "assert n//2 != 0, f\"Only even number of data allowed (current {n})\"\n",
        "\n",
        "x = np.concatenate([np.linspace(-x_max, -x_min_pos, num=n//2), np.linspace(x_min_pos, x_max, num=n//2)])\n",
        "y = np.sin(x)\n",
        "x += np.random.normal(0, 0.1, x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TRdRtre-5-i"
      },
      "source": [
        "... and the \"expanded\" test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGlzWeL7VwWW"
      },
      "outputs": [],
      "source": [
        "x_test_max = 7\n",
        "n_test = 1000\n",
        "x_test = np.linspace(-x_test_max, x_test_max, 1000)\n",
        "y_test = np.sin(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIiG_OalZGy8"
      },
      "source": [
        "## MC-Dropout\n",
        "\n",
        "MC-Dropout proposes to extend the usage of Dropout, a regularization method which acts by inhibiting (i.e., zeroing-out) neurons in a given layer with a pre-specified probability $p_l$, which can be different for each layer $l$ where Dropout is applied:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1YJHNYKk9ff9-DuX8XOnYTH4xcxsIEt85)\n",
        "\n",
        "The neurons to be inhibited are re-drawn for each forward pass.\n",
        "\n",
        "Normally, Dropout has two behaviors:\n",
        "* During training time, the aforementioned random inhibition is run for each forward pass.\n",
        "* During eval (test) time, there is no inhibition, but the activation for each neuron of each layer where Dropout is applied is _rescaled_ by the dropout probability $p_l$.\n",
        "\n",
        "The behavior during training time essentially acts as a stochastic _device_, enabling a frequentist NN to act as a Bayesian.\n",
        "When the training behavior is applied also during eval time, we talk about _MonteCarlo (MC) Dropout_: the idea is that, instead of producing a deterministic output, we allow for the model to have a _distribution_ over the weights (which is essentially a combination of a multidimensional Bernoulli over the parameters of the frequentist model) which allows us to sample multiple outputs for a given input, thus enabling us to aggregate the predictions and to produce rough estimates of uncertainty.\n",
        "\n",
        "Below, you will find an implementation of an MC Dropout layer in Keras which can be plugged into any Keras model (immediately after the layer which we want Dropout to act on) to turn the model from deterministic to probabilistic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGkcSapIBdF4"
      },
      "outputs": [],
      "source": [
        "class StochasticDropout(keras.layers.Dropout):\n",
        "    \"\"\"\n",
        "        Applies Dropout to the input, independent of the training phase.\n",
        "\n",
        "        Used to easily implement MC-Dropout. It is a drop-in replacement for\n",
        "        the standard Keras Dropout layer, but note that this layer applies\n",
        "        dropout at the training and inference phases.\n",
        "    \"\"\"\n",
        "    def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n",
        "        super(StochasticDropout, self).__init__(rate, noise_shape, seed, **kwargs)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if 0. < self.rate < 1.:\n",
        "            noise_shape = self._get_noise_shape(inputs)\n",
        "\n",
        "            return K.dropout(inputs, self.rate, noise_shape, seed=self.seed)\n",
        "\n",
        "        return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUwfrw9KEexr"
      },
      "source": [
        "**Q: _why can't we use a normal Dropout implementation, like `K.Dropout`, in our Keras model?_**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Juyxd1on7rba"
      },
      "source": [
        "#### Notes on the implementation\n",
        "\n",
        "In the following cells, we will be using this nomenclature:\n",
        "* The architecture of the BNNs will be called \"backbone\"\n",
        "* The backbone will be wrapped into a \"regressor\" structure, which implements under-the-hood MC sampling during the eval mode. These structures come already pre-implemented within `keras_uncertainty`.\n",
        "\n",
        "All of the layers which we will implement during this lab are also taken from `keras_uncertainty` source code (with minimal changes); all of them can also be found as member of the `layers` submodule: they are re-implemented here for the sake of clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7O3dVXABp28"
      },
      "outputs": [],
      "source": [
        "dropout_p = 0.2\n",
        "\n",
        "backbone_dropout = keras.models.Sequential([\n",
        "    keras.Input(shape=(1,)),\n",
        "    keras.layers.Dense(units=16, activation=\"relu\"),\n",
        "    StochasticDropout(dropout_p),\n",
        "    keras.layers.Dense(units=16, activation=\"relu\"),\n",
        "    StochasticDropout(dropout_p),\n",
        "    keras.layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "backbone_dropout.compile(loss=\"mean_squared_error\", optimizer=\"adam\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWIYrnJO8svJ"
      },
      "source": [
        "Train the backbone on our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOEbhkUjESH-",
        "outputId": "6777638a-30e4-4907-ea2b-c96195d8fd4c"
      },
      "outputs": [],
      "source": [
        "backbone_dropout.fit(x, y, verbose=2, epochs=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo_AfEqjFP81"
      },
      "source": [
        "Let's obtain the predictions...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QfeqEk7FGdQ",
        "outputId": "5ea77943-e656-4bb2-c01a-55ff3ec3f53b"
      },
      "outputs": [],
      "source": [
        "print(backbone_dropout.predict(np.array([0,1])))\n",
        "\n",
        "print(backbone_dropout.predict(np.array([0,1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvPNeIVnFTK1"
      },
      "source": [
        "**Q: _Notice that the predictions change despite the two inputs being identical. Why that?_**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQwP62ALFTGW"
      },
      "source": [
        "Wrap the model in a `StochasticRegressor` structure to sample 25 outputs per test datapoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P--uHj9LFSnF"
      },
      "outputs": [],
      "source": [
        "model_dropout = ku.models.StochasticRegressor(backbone_dropout)\n",
        "y_pred_dropout = model_dropout.predict(x_test, 25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtpUnivs_C3B"
      },
      "source": [
        "How is the prediction returned? Let's find out..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K7vq1WjVjfK",
        "outputId": "f97c8e94-6ec7-4433-db25-76dae7a8639e"
      },
      "outputs": [],
      "source": [
        "print(type(y_pred_dropout))\n",
        "print(len(y_pred_dropout))\n",
        "print(y_pred_dropout[0].shape, y_pred_dropout[1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adWhf9dO_TTE"
      },
      "source": [
        "The `StochasticRegressor` structure already post-processed the prediction into mean and std, which lets us reason in terms of uncertainty.\n",
        "\n",
        "**Q: _on which dimension(s) (mc sample, test datapoint, output dimension) are the averages obtained?_**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcLnUjumWKFr"
      },
      "outputs": [],
      "source": [
        "y_pred_dropout_mean, y_pred_dropout_std = y_pred_dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo6bA_yi__sA"
      },
      "source": [
        "Let us prepare a function (`plot_prediction_with_uncertainty`) to plot these outputs incorporating the model uncertainty into the process. We can re-use the function from the intro with minimal modifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I__oD7gUWOUn"
      },
      "outputs": [],
      "source": [
        "def plot_prediction_with_uncertainty(x, y_true, y_pred_mean, y_pred_std, x_max, x_min_pos):\n",
        "  plt.scatter(x, y_true, s=5)\n",
        "  plt.scatter(x, y_pred_mean, s=5)\n",
        "  uncertainty_area_min = y_pred_mean - 2*y_pred_std\n",
        "  uncertainty_area_max = y_pred_mean + 2*y_pred_std\n",
        "  plt.fill_between(x_test.squeeze(), uncertainty_area_min, uncertainty_area_max, alpha=.25)\n",
        "\n",
        "  yline = (\n",
        "      min(y_true.min().item(), uncertainty_area_min.min().item()),\n",
        "      max(y_true.max().item(), uncertainty_area_max.max().item())\n",
        "  )\n",
        "  plt.vlines((-x_max, -x_min_pos, x_min_pos, x_max), ymin=yline[0], ymax=yline[1], colors=\"black\")\n",
        "  x1 = np.array((-x_max, -x_min_pos))\n",
        "  x2 = np.array((x_min_pos, x_max))\n",
        "  y1 = np.array([yline[1], yline[1]])\n",
        "  y2 = np.array([yline[0], yline[0]])\n",
        "  plt.fill_between(x1, y1, y2, where=(y1 > y2), color=\"yellow\", alpha=.3)\n",
        "  plt.fill_between(x2, y1, y2, where=(y1 > y2), color=\"yellow\", alpha=.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "MrB8K8hUXPf9",
        "outputId": "5afb25d0-6371-431c-cb14-420dda317ed7"
      },
      "outputs": [],
      "source": [
        "plot_prediction_with_uncertainty(x_test, y_test, y_pred_dropout_mean.squeeze(), y_pred_dropout_std.squeeze(), x_max, x_min_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en9xhrDFAbgR"
      },
      "source": [
        "Let us _package_ the procedure we have used for MC Dropout in a function: we'll be able to re-use it for (almost) all the other methods within this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cjq5Y5l8coo3"
      },
      "outputs": [],
      "source": [
        "def training_and_testing_pipeline(backbone, n_epochs, n_samples_test=25):\n",
        "  backbone.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
        "  backbone.fit(x, y, epochs=n_epochs, verbose=2)\n",
        "  model = ku.models.StochasticRegressor(backbone)\n",
        "  print(f\"Predicting {n_samples_test} samples per test datapoint...\")\n",
        "  y_pred_mean, y_pred_std = model.predict(x_test, n_samples_test)\n",
        "  plot_prediction_with_uncertainty(x_test, y_test, y_pred_mean.squeeze(), y_pred_std.squeeze(), x_max, x_min_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD2htf_fZJ_r"
      },
      "source": [
        "## MC-DropConnect\n",
        "\n",
        "MC-DropConnect is an alternate approach to infusing stochasticity into a deterministic model: instead of inhibiting neurons' activations, we randomly inhibit _synapses_ (i.e., single connections between neurons) with a per-layer probability $p_l$:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1pRrJhFQpQBmyYzbwl24IOb_Avj9hBLxI)\n",
        "\n",
        "Despite the striking similarity between Dropout and DropConnect, the implementation of DropConnect does not happen as a standalone module which we can _insert_ after a Dense/Linear layer: rather, we have to re-implement (or extend) this structure to accomodate this device.\n",
        "This is due to a technicality. Let us denote with $x$ the input to the layer, $W$ its weights, $b$ its bias, $\\sigma$ its activation function, $\\hat{y}$ the output of the layer. $p$ indicates the Dropout/DropConnect probability.\n",
        "* Dropout happens **after** the main calculation of the layer has been carried out\n",
        "  $$\n",
        "  \\hat{y} = \\text{Dropout}_{p}(\\sigma(x^\\top W+b))\n",
        "  $$\n",
        "  Alternatively, we might have:\n",
        "  $$\n",
        "  \\hat{y} = \\sigma(\\text{Dropout}_{p}(x^\\top W+b))\n",
        "  $$\n",
        "  This does not really matter: we can always place a Dropout layer after the Dense.\n",
        "* DropConnect acts **during** the main calculation:\n",
        "  $$\n",
        "  \\hat{y} = \\sigma(x^\\top \\text{DropConnect}_{p}(W)+b)\n",
        "  $$\n",
        "  If we want to implement it, thus, we must **_insert_** it into a Dense layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shS3Q_e7X3x6"
      },
      "outputs": [],
      "source": [
        "class DropConnect:\n",
        "    def __init__(self, prob=0.5, drop_bias=False, noise_shape=None):\n",
        "        self.prob = prob\n",
        "        self.drop_bias = drop_bias\n",
        "        self.noise_shape = noise_shape\n",
        "        self.kernel_noise_shape = None\n",
        "        self.bias_noise_shape = None\n",
        "\n",
        "    @property\n",
        "    def needs_drop(self):\n",
        "        return 0.0 < self.prob < 1.0\n",
        "\n",
        "    def sample(self, tensor, dropit=True, noise_shape=None):\n",
        "        if dropit:\n",
        "            return K.dropout(tensor, self.prob, noise_shape)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def replace_tensor(self, tensor_train, tensor_test):\n",
        "        if self.uses_learning_phase:\n",
        "            return K.in_train_phase(tensor_train, tensor_test)\n",
        "        else:\n",
        "            return tensor_train\n",
        "\n",
        "    def get_noise_shape(self, inputs):\n",
        "        if self.noise_shape is None:\n",
        "            return self.noise_shape\n",
        "\n",
        "        symbolic_shape = K.shape(inputs)\n",
        "        noise_shape = [symbolic_shape[axis] if shape is None else shape\n",
        "                       for axis, shape in enumerate(self.noise_shape)]\n",
        "\n",
        "        return tuple(noise_shape)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            \"prob\": self.prob,\n",
        "            \"drop_bias\": self.drop_bias,\n",
        "            \"noise_shape\": self.noise_shape\n",
        "        }\n",
        "\n",
        "        return config\n",
        "\n",
        "class DropConnectDense(DropConnect, keras.layers.Dense):\n",
        "    def __init__(self, units, prob=0.5, drop_bias=False, noise_shape=None, use_learning_phase = False, **kwargs):\n",
        "        DropConnect.__init__(self, prob=prob, drop_bias=drop_bias, noise_shape=noise_shape)\n",
        "        keras.layers.Dense.__init__(self, units, **kwargs)\n",
        "\n",
        "        if self.needs_drop:\n",
        "            self.uses_learning_phase = use_learning_phase\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        kernel_sample = self.sample(self.kernel)\n",
        "        bias_sample = self.sample(self.bias, dropit=self.drop_bias)\n",
        "\n",
        "        outputs = K.dot(inputs, kernel_sample)\n",
        "\n",
        "        if self.use_bias:\n",
        "            outputs += bias_sample\n",
        "\n",
        "        # This always produces stochastic outputs\n",
        "        return self.activation(outputs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config_dc = DropConnect.get_config(self)\n",
        "        config_base = keras.layers.Dense.get_config(self)\n",
        "\n",
        "        return dict(list(config_dc.items()) + list(config_base.items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPXA2m-ID2y2"
      },
      "source": [
        "**Q: _why are we using Dropout in DropConnect? Isn't one supposed to drop neurons, while the other drops weights?_**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DhJEFmoaV3-"
      },
      "outputs": [],
      "source": [
        "dropconnect_p = 0.05\n",
        "\n",
        "backbone_dropconnect = keras.Sequential([\n",
        "    keras.Input(shape=(1,)),\n",
        "    DropConnectDense(16, prob=dropconnect_p, activation=\"relu\"),\n",
        "    DropConnectDense(16, prob=dropconnect_p, activation=\"relu\"),\n",
        "    DropConnectDense(1, prob=dropconnect_p, ),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yMcUPGo5bPGp",
        "outputId": "cb609bc9-faf9-4ebe-ccb5-e0a8b7e3f19d"
      },
      "outputs": [],
      "source": [
        "training_and_testing_pipeline(\n",
        "    backbone_dropconnect,\n",
        "    n_epochs=750,\n",
        "    n_samples_test=25\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZZ9p-KweuO6"
      },
      "source": [
        "## Deep Ensembles\n",
        "\n",
        "Deep Ensembles are sets of Deep NNs (DNNs) which are tasked with producing multiple predictions for the same datapoint. The rationale is that the inherent randomness contained within the initialization and training process will cause the network to find different sets of parameters solving the same task, while producing possibly (widely) different responses to areas of high uncertainty (e.g., out-of-distribution data), thus finding an implicit way to estimate uncertainty even with deterministic models.\n",
        "\n",
        "In this specific lab, we will produce ensemble of two-headed DNNs, one of which estimates the mean value, and the other estimates the variance; thus, the training process will vary, since we will have to use the Gaussian Negative Log-Likelihood loss instead of the canonical MSE we have insofar utilized.\n",
        "\n",
        "In addition, we cannot use the `StochasticRegressor` structure we have previously made use of: this structure is a wrapper for single-component stochastic models, while Deep Ensembles are formed of multiple, separate models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X0cPvrMet8j",
        "outputId": "5d9b1d5a-c12a-4713-ea75-9c13093eb901"
      },
      "outputs": [],
      "source": [
        "def regression_gaussian_nll_loss(variance_tensor, epsilon=1e-8, variance_logits=False):\n",
        "    \"\"\"\n",
        "        Gaussian negative log-likelihood for regression, with variance estimated by the model.\n",
        "        This function returns a keras regression loss, given a symbolic tensor for the sigma square output of the model.\n",
        "        The training model should return the mean, while the testing/prediction model should return the mean and variance.\n",
        "    \"\"\"\n",
        "    def nll(y_true, y_pred):\n",
        "        #if variance_logits:\n",
        "        #    variance_tensor = K.exp(variance_tensor)\n",
        "\n",
        "        return 0.5 * K.mean(K.log(variance_tensor + epsilon) + K.square(y_true - y_pred) / (variance_tensor + epsilon))\n",
        "\n",
        "    return nll\n",
        "\n",
        "def ensemble_component_initializer():\n",
        "    inp = keras.Input(shape=(1,))\n",
        "    x = keras.layers.Dense(32, activation=\"relu\")(inp)\n",
        "    x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
        "    mean = keras.layers.Dense(1)(x)\n",
        "    var = keras.layers.Dense(1, activation=\"softplus\")(x)\n",
        "\n",
        "    train_model = keras.Model(inp, mean)\n",
        "    pred_model = keras.Model(inp, [mean, var])\n",
        "\n",
        "    train_model.compile(loss=regression_gaussian_nll_loss(var), optimizer=\"adam\")\n",
        "\n",
        "    return train_model, pred_model\n",
        "\n",
        "n_components = 10\n",
        "n_epochs = 500\n",
        "ensemble = [ensemble_component_initializer() for _ in range(n_components)]\n",
        "\n",
        "for index, (component, _) in enumerate(ensemble):\n",
        "  print(f\"--- TRAINING COMPONENT {index+1} OF THE ENSEMBLE ---\")\n",
        "  component.fit(x, y, epochs=n_epochs, verbose=2)\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Afh-up47F0Bj"
      },
      "source": [
        "**Q: _in the code above, why did we write the comprehension in this way: `for index, (component, _) in enumerate(ensemble)`?_**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "8INpiuapjjj2",
        "outputId": "95665c47-077c-474f-b85d-45a425b03cd5"
      },
      "outputs": [],
      "source": [
        "ensemble_means = []\n",
        "ensemble_variances = []\n",
        "\n",
        "for _, test_component in ensemble:\n",
        "  mean, var = test_component.predict(x_test)\n",
        "  ensemble_means.append(mean)\n",
        "  ensemble_variances.append(var)\n",
        "\n",
        "ensemble_mean = np.mean(ensemble_means, axis=0)\n",
        "ensemble_var = (np.mean(ensemble_variances + np.square(ensemble_means), axis=0) - np.square(ensemble_mean)).clip(0.0)\n",
        "ensemble_std = np.sqrt(ensemble_var)\n",
        "\n",
        "plot_prediction_with_uncertainty(x_test, y_test, ensemble_mean.squeeze(), ensemble_std.squeeze(), x_max, x_min_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSc5Fc710eDU"
      },
      "source": [
        "Tip: You may also use the structure `models.DeepEnsembleRegressor` from `keras_uncertainty` to help with the training/testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2Kp_79V1L84"
      },
      "source": [
        "## Variational Inference (VI) - BayesByBackprop\n",
        "\n",
        "VI takes a different approach to training BNNs.\n",
        "Instead of using tricks to modify deterministic layers into stochastic ones, it uses a full Bayesian approach, which consists in defining a _prior distribution_ on the parameters $\\Theta \\sim P(\\Theta)$ and computing a _posterior distribution_ $P(\\Theta\\vert \\mathcal{D})$, which equates to an _update in knowledge_ concerning the parameters $\\Theta$ following the observed data $\\mathcal{D}$.\n",
        "The information conveyed by the data, which showcases the empirical \"plausibility\" of the data according to the current set of parameters is the _likelihood_ $P(\\mathcal{D}\\vert \\Theta)$, which is usually the ideological \"driver\" toward the deterministic ML models: optimize parameters to maximize the likelihood.\n",
        "\n",
        "Using Bayes' theorem, we can find a rule for updating the prior distribution according to the evidence brought on by the likelihood:\n",
        "\n",
        "$$\n",
        "P(\\Theta \\vert \\mathcal{D}) = \\frac{P(\\mathcal{D}\\vert \\Theta)\\cdot P{\\Theta}}{P(\\mathcal{D})}\n",
        "$$\n",
        "\n",
        "The hard part of the computation above is the denominator, which would require to either\n",
        "* know the data generating process (which is usually unknown in ML applications), or\n",
        "* calculate a very computationally-intensive integral, which obtains $P(\\mathcal{D})$ from _marginalization_ of the likelihood.\n",
        "\n",
        "There are several ways to solve this problem in an approximate way, VI being one of them.\n",
        "The foundational idea is to find an approximation of the posterior, which we call _variational posterior_ ($q$), from which we can sample the parameters of the model and obtain a prediction.\n",
        "The variational posterior is trained by trying to _minimize_ the Kullback-Leibler (KL) divergence between itself and the true posterior.\n",
        "\n",
        "Given the unknown nature of the true posterior, the KL divergence is approximated by the following loss function:\n",
        "$$\n",
        "\\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^{n} \\left[ \\log \\underbrace{q(\\theta_i)}_{\\text{var. post.}} - \\log \\underbrace{P(\\theta_i)}_{\\text{prior}} - \\log \\underbrace{P(\\mathcal{D}\\vert \\theta_i)}_{\\text{likelihood}} \\right]\n",
        "$$\n",
        "The $\\theta_i$'s in the formula are samples from the variational posterior: the loss is hence a mean over multiple of these samples.\n",
        "\n",
        "The immediate architectural difference between previously seen NNs is that we now have to **explicitly** represent the distribution $q$ for each of the layers. The most common choice is to use a Gaussian distribution, which requires the layers to be parameterized by a mean $\\mu$ and a variance $\\Sigma$, which is usually a diagonal matrix (→ each parameter of the layer is parameterized by a scalar mean and a scalar variance).\n",
        "A backward pass would calculate the gradient for all of these parameters: hence, at each step of (S)GD we update both the mean and variance.\n",
        "Note the following:\n",
        "* To allow for the differentiability of the loss function w.r.t. the parameters $\\mu$ and $\\Sigma$, the sampling process is replaced by the _reparameterization trick_: instead of sampling directly from $q$, we take $\\mu$ and add to it $\\Sigma$ times a random sample from a standard Gaussian. Since the source of randomness is independent from $\\mu, \\Sigma$, it gets eliminated in the differentiation process and we can train the parameters via GD.\n",
        "* Since GD could cause the values of the diagonals of $\\Sigma$ to fall below 0, we elect a vector $\\rho$ as a replacement for the training process, and get $\\text{diag}(\\Sigma) = \\frac{1}{1+e^{-\\rho}} = \\text{softplus}(\\rho)$. The softplus function transforms a real number into a positive number, thus ensuring that the diagonal of the variance matrix will always contain positive scalars.\n",
        "\n",
        "The [paper](https://arxiv.org/pdf/1505.05424.pdf) introducing this type of VI-based training calls this process BayesByBackprop.\n",
        "Its authors propose adopting a mixture-of-Gaussians prior with 2 component.\n",
        "These components have 0 mean and two different diagonal variance matrices $\\Sigma_1 = \\sigma_1 I$ and $\\Sigma_2 = \\sigma_2 I$, with $\\sigma_1, \\sigma_2 \\in \\mathbb{R}^+$.\n",
        "The prevalence of component 1 w.r.t. component 2 is regulated by a scalar $pi\\in[0,1]$.\n",
        "For each parameter $\\theta^{(j)} \\in \\Theta$, we have the following prior\n",
        "\n",
        "$$\n",
        "P(\\theta^{(j)}) = \\pi [\\mathcal{N}(0, \\sigma_1)(\\theta^{(j)})] + (1-\\pi) [\\mathcal{N}(0, \\sigma_2)(\\theta^{(j)})].\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG51gbad0O8w"
      },
      "outputs": [],
      "source": [
        "class VariationalDense(keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 units,\n",
        "                 kl_weight,\n",
        "                 activation=None,\n",
        "                 initializer_sigma=0.1,\n",
        "                 prior=True,\n",
        "                 prior_sigma_1=1.0,\n",
        "                 prior_sigma_2=0.5,\n",
        "                 prior_pi=0.5,\n",
        "                 bias_distribution=False,\n",
        "                 **kwargs):\n",
        "        self.units = units\n",
        "        self.kl_weight = kl_weight\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.initializer_sigma = initializer_sigma\n",
        "        self.prior = prior\n",
        "        self.prior_sigma_1 = prior_sigma_1\n",
        "        self.prior_sigma_2 = prior_sigma_2\n",
        "        self.prior_pi_1 = prior_pi\n",
        "        self.prior_pi_2 = 1.0 - prior_pi\n",
        "        self.init_sigma = np.sqrt(self.prior_pi_1 * self.prior_sigma_1 ** 2 +\n",
        "                                  self.prior_pi_2 * self.prior_sigma_2 ** 2)\n",
        "        self.bias_distribution = bias_distribution\n",
        "        self.uses_learning_phase = True\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return [(None, self.units)]\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        feature_dims = input_shape[-1]\n",
        "        self.kernel_mu = self.add_weight(name='kernel_mu',\n",
        "                                         shape=(feature_dims, self.units),\n",
        "                                         initializer=keras.initializers.normal(stddev=self.initializer_sigma),\n",
        "                                         trainable=True)\n",
        "        self.bias_mu = self.add_weight(name='bias_mu',\n",
        "                                       shape=(self.units,),\n",
        "                                       initializer=keras.initializers.normal(stddev=self.initializer_sigma),\n",
        "                                       trainable=True)\n",
        "        self.kernel_rho = self.add_weight(name='kernel_rho',\n",
        "                                          shape=(feature_dims, self.units),\n",
        "                                          initializer=keras.initializers.normal(mean=-3.0, stddev=self.initializer_sigma),\n",
        "                                          trainable=True)\n",
        "        self.bias_rho = self.add_weight(name='bias_rho',\n",
        "                                        shape=(self.units,),\n",
        "                                        initializer=keras.initializers.normal(mean=-3.0, stddev=self.initializer_sigma),\n",
        "                                        trainable=self.bias_distribution)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        kernel_sigma = K.softplus(self.kernel_rho)\n",
        "        kernel = self.kernel_mu + kernel_sigma * K.random_normal(self.kernel_mu.shape)\n",
        "\n",
        "        bias_sigma = K.softplus(self.bias_rho)\n",
        "        bias = self.bias_mu + bias_sigma * K.random_normal(self.bias_mu.shape)\n",
        "\n",
        "        loss = self.kl_loss(kernel, self.kernel_mu, kernel_sigma) + self.kl_loss(bias, self.bias_mu, bias_sigma)\n",
        "\n",
        "        self.add_loss(K.in_train_phase(loss, 0.0))\n",
        "\n",
        "        # This always produces stochastic outputs\n",
        "        return self.activation(K.dot(inputs, kernel) + bias)\n",
        "\n",
        "    def kl_loss(self, w, mu, sigma):\n",
        "        return self.kl_weight * K.mean(ku.distributions.gaussian.log_probability(w, mu, sigma) - self.prior * self.log_prior_prob(w))\n",
        "\n",
        "    def log_prior_prob(self, w):\n",
        "        return K.log(self.prior_pi_1 * ku.distributions.gaussian.probability(w, 0.0, self.prior_sigma_1) +\n",
        "                     self.prior_pi_2 * ku.distributions.gaussian.probability(w, 0.0, self.prior_sigma_2))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'kl_weight': self.kl_weight,\n",
        "                  'activation': self.activation.__name__,\n",
        "                  #'bias': self.bias,\n",
        "                  'prior': self.prior,\n",
        "                  'prior_sigma_1': self.prior_sigma_1,\n",
        "                  'prior_sigma_2': self.prior_sigma_2,\n",
        "                  'prior_pi_1': self.prior_pi_1}\n",
        "        base_config = super(VariationalDense, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EqT2FrMRnjs"
      },
      "outputs": [],
      "source": [
        "n_plus = 1000\n",
        "assert n//2 != 0, f\"Only even number of data allowed (current {n})\"\n",
        "\n",
        "x_plus = np.concatenate([np.linspace(-x_max, -x_min_pos, num=n_plus//2), np.linspace(x_min_pos, x_max, num=n_plus//2)])\n",
        "y_plus = np.sin(x)\n",
        "x_plus += np.random.normal(0, 0.1, x_plus.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IGVnXgJ11Xll",
        "outputId": "3d52ac0f-f529-4a43-b4cd-69c66e41fa7b"
      },
      "outputs": [],
      "source": [
        "num_neurons = 16\n",
        "batch_size = 32\n",
        "num_batches = np.ceil((len(x)*100)/batch_size)\n",
        "n_epochs = 200\n",
        "\n",
        "backbone_vi = keras.Sequential([\n",
        "    keras.Input((1,)),\n",
        "    VariationalDense(num_neurons, kl_weight=1/num_batches, activation=\"relu\"),\n",
        "    VariationalDense(num_neurons, kl_weight=1/num_batches, activation=\"relu\"),\n",
        "    VariationalDense(1, kl_weight=1/num_batches),\n",
        "])\n",
        "\n",
        "training_and_testing_pipeline(backbone_vi, n_epochs, n_samples_test=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HjgjdIBuIx4"
      },
      "source": [
        "## Flipout\n",
        "\n",
        "Flipout aims at tackling one of VI's main issues: the necessity to produce multiple samples per datapoint.\n",
        "It builds upon BayesByBackprop by introducing a per-datapoint perturbation to the parameters sampled from the variational posterior.\n",
        "\n",
        "BayesByBackprop had the following procedure for sampling (reparameterization trick):\n",
        "$$\n",
        "\\theta^{(j)} = \\mu^{(j)} + \\underbrace{\\sigma^{(j)} \\cdot \\mathcal{N}(0,1)}_{\\text{perturbation } \\delta_{\\theta^{(j)}}}\n",
        "$$\n",
        "This perturbation is the same for all datapoints $i \\in \\{1,\\dots,n\\}$.\n",
        "\n",
        "Flipout introduces a per-datapoint perturbation by randomly flipping the sign of the input incoming to the layer and its output (equivalent to directly perturbing the parameters).\n",
        "If we suppose the layer having no bias, we have, for datapoint $i$:\n",
        "$$\n",
        "y_i = \\text{Activation}\\left(\\underbrace{r_i}_{q\\times 1} \\odot \\left[\\underbrace{\\Theta}_{q\\times p} \\underbrace{(x_i \\odot s_i)^\\top}_{p\\times 1} \\right]\\right)\n",
        "$$\n",
        "Where $r_i$ and $s_i$ are samples from the Rademacher distribution (probability distribution which assigns equal probability to scalars -1 and 1).\n",
        "The effect of $r_i$ and $s_i$ is to effectively create different per-datapoint perturbations, speeding up VI and rendering it more stable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igYfiMhcuH1f"
      },
      "outputs": [],
      "source": [
        "class FlipoutDense(keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 units,\n",
        "                 kl_weight,\n",
        "                 activation=None,\n",
        "                 initializer_sigma=0.1,\n",
        "                 prior=True,\n",
        "                 prior_sigma_1=1.5,\n",
        "                 prior_sigma_2=0.1,\n",
        "                 prior_pi=0.5,\n",
        "                 bias_distribution=False,\n",
        "                  **kwargs):\n",
        "        self.units = units\n",
        "        self.kl_weight = kl_weight\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.prior = prior\n",
        "        self.prior_sigma_1 = prior_sigma_1\n",
        "        self.prior_sigma_2 = prior_sigma_2\n",
        "        self.prior_pi_1 = prior_pi\n",
        "        self.prior_pi_2 = 1.0 - prior_pi\n",
        "        self.initializer_sigma = initializer_sigma\n",
        "        self.uses_learning_phase = True\n",
        "        self.bias_distribution = bias_distribution\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return [(None, self.units)]\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        feature_dims = input_shape[-1]\n",
        "        self.kernel_mu = self.add_weight(name='kernel_mu',\n",
        "                                         shape=(feature_dims, self.units),\n",
        "                                         initializer=keras.initializers.normal(stddev=self.initializer_sigma),\n",
        "                                         trainable=True)\n",
        "\n",
        "        # -3.0 is an approximation for 0.0 with softplus, softplus(-3.0) ~ 0.0\n",
        "        self.kernel_rho = self.add_weight(name='kernel_rho',\n",
        "                                          shape=(feature_dims, self.units),\n",
        "                                          initializer=keras.initializers.normal(mean=-3.0, stddev=self.initializer_sigma),\n",
        "                                          trainable=True)\n",
        "\n",
        "        self.bias_mu = self.add_weight(name='bias_mu',\n",
        "                                       shape=(self.units,),\n",
        "                                       initializer=keras.initializers.normal(stddev=self.initializer_sigma),\n",
        "                                       trainable=True)\n",
        "\n",
        "        self.bias_rho = self.add_weight(name='bias_rho',\n",
        "                                        shape=(self.units,),\n",
        "                                        initializer=keras.initializers.normal(mean=-3.0, stddev=self.initializer_sigma),\n",
        "                                        trainable=self.bias_distribution)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        kernel_sigma = K.softplus(self.kernel_rho)\n",
        "        kernel_perturb = kernel_sigma * K.random_normal(self.kernel_mu.shape)\n",
        "        kernel = self.kernel_mu + kernel_perturb\n",
        "\n",
        "        if self.bias_distribution:\n",
        "            bias_sigma = K.softplus(self.bias_rho)\n",
        "            bias = self.bias_mu + bias_sigma * K.random_normal(self.bias_mu.shape)\n",
        "        else:\n",
        "            bias = self.bias_mu\n",
        "\n",
        "        loss = self.kl_loss(kernel, self.kernel_mu, kernel_sigma)\n",
        "\n",
        "        if self.bias_distribution:\n",
        "            loss += self.kl_loss(bias, self.bias_mu, bias_sigma)\n",
        "\n",
        "        self.add_loss(K.in_train_phase(loss, 0.0))\n",
        "\n",
        "        input_shape = K.shape(inputs)\n",
        "        batch_shape = input_shape[:-1]\n",
        "        sign_input = ku.distributions.rademacher.sample(input_shape)\n",
        "        sign_output = ku.distributions.rademacher.sample(K.concatenate([batch_shape, K.expand_dims(self.units, 0)], axis=0))\n",
        "        perturbed_inputs = K.dot(inputs * sign_input, kernel_perturb) * sign_output\n",
        "\n",
        "        outputs = K.dot(inputs, self.kernel_mu)\n",
        "        outputs += perturbed_inputs\n",
        "        outputs += bias\n",
        "\n",
        "        # This always produces stochastic outputs\n",
        "        return self.activation(outputs)\n",
        "\n",
        "    def kl_loss(self, w, mu, sigma):\n",
        "        return self.kl_weight * K.mean(ku.distributions.gaussian.log_probability(w, mu, sigma) - self.prior * self.log_prior_prob(w))\n",
        "\n",
        "    def log_prior_prob(self, w):\n",
        "        return K.log(self.prior_pi_1 * ku.distributions.gaussian.probability(w, 0.0, self.prior_sigma_1) +\n",
        "                     self.prior_pi_2 * ku.distributions.gaussian.probability(w, 0.0, self.prior_sigma_2))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'kl_weight': self.kl_weight,\n",
        "                  'activation': self.activation.__name__,\n",
        "                  #'bias': self.bias,\n",
        "                  'prior': self.prior,\n",
        "                  'prior_sigma_1': self.prior_sigma_1,\n",
        "                  'prior_sigma_2': self.prior_sigma_2,\n",
        "                  'prior_pi': self.prior_pi_1}\n",
        "        base_config = super(FlipoutDense, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZGmFcwGguVLN",
        "outputId": "5736e753-98a0-493a-b032-556d1032290a"
      },
      "outputs": [],
      "source": [
        "num_neurons = 16\n",
        "batch_size = 32\n",
        "num_batches = np.ceil((len(x)*100)/batch_size)\n",
        "n_epochs = 700\n",
        "\n",
        "backbone_flip = keras.Sequential([\n",
        "    keras.Input((1,)),\n",
        "    FlipoutDense(num_neurons, kl_weight=1/num_batches, activation=\"relu\"),\n",
        "    FlipoutDense(num_neurons, kl_weight=1/num_batches, activation=\"relu\"),\n",
        "    FlipoutDense(1, kl_weight=1/num_batches),\n",
        "])\n",
        "\n",
        "training_and_testing_pipeline(backbone_flip, n_epochs, n_samples_test=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EFRqKQ1EaHI"
      },
      "source": [
        "## The task for today\n",
        "\n",
        "Split in groups. Each group is to be assigned one of the methods here implemented. Your goal is to train a BNN on MNIST using one of these methods.\n",
        "\n",
        "Tips:\n",
        "* You may download the dataset MNIST using this command\n",
        "\n",
        "  `(x, y), (x_test, y_test) = tf.keras.datasets.mnist.load_data()`\n",
        "  \n",
        "  Be aware that the images will have dtype `uint8` and the pixel values will range from 0 to 255. For a NN to train well, you should convert them to `float32` and have them in the range [0, 1] or [-1, 1] or any interval close to these values.\n",
        "* Until now, we have used the `StochasticRegressor` structure from `keras_uncertainty`. This may not be the optimal choice for MNIST..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMAwEwWFUnSSGAzRmGyJm86",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
